%This is the change file for the original Docker's Documentation file.
%This is part of Japanese translation version for Docker's Documantation.

@x
description: Containerize RAG application using Ollama and Docker
keywords: python, generative ai, genai, llm, ollama, rag, qdrant
title: Build a RAG application using Ollama and Docker
linkTitle: RAG Ollama application
summary: |
  This guide demonstrates how to use Docker to deploy Retrieval-Augmented
  Generation (RAG) models with Ollama.
@y
description: Ollama と Docker を使った RAG アプリケーションのコンテナー化
keywords: python, generative ai, genai, llm, ollama, rag, qdrant
title: Ollama と Docker を使った RAG アプリのビルド
linkTitle: RAG Ollama application
summary: |
  このガイドでは Ollama を使った RAG (Retrieval-Augmented Generation) モデル を Docker によってデプロイする方法について説明します。
@z

@x
tags: [ai]
@y
tags: [ai]
@z

% params:
@x
  time: 20 minutes
@y
  time: 20 分
@z

@x
The Retrieval Augmented Generation (RAG) guide teaches you how to containerize an existing RAG application using Docker. The example application is a RAG that acts like a sommelier, giving you the best pairings between wines and food. In this guide, you’ll learn how to:
@y
The Retrieval Augmented Generation (RAG) guide teaches you how to containerize an existing RAG application using Docker. The example application is a RAG that acts like a sommelier, giving you the best pairings between wines and food. In this guide, you’ll learn how to:
@z

@x
- Containerize and run a RAG application
- Set up a local environment to run the complete RAG stack locally for development
@y
- Containerize and run a RAG application
- Set up a local environment to run the complete RAG stack locally for development
@z

@x
Start by containerizing an existing RAG application.
@y
Start by containerizing an existing RAG application.
@z
